{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import library\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\ntry:\n  %tensorflow_version 2.x\nexcept Exception:\n  pass\n   \nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow_datasets as tfds\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\n\nimport os\nprint(tf.__version__)\n ","metadata":{"execution":{"iopub.status.busy":"2021-05-22T22:49:31.732984Z","iopub.execute_input":"2021-05-22T22:49:31.733741Z","iopub.status.idle":"2021-05-22T22:49:39.133149Z","shell.execute_reply.started":"2021-05-22T22:49:31.733641Z","shell.execute_reply":"2021-05-22T22:49:39.132182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import dataset\n\nimport pandas as pd\ntrain_data = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\ntest_data = pd.read_csv('../input/quora-insincere-questions-classification/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T22:49:48.094744Z","iopub.execute_input":"2021-05-22T22:49:48.096303Z","iopub.status.idle":"2021-05-22T22:49:54.979959Z","shell.execute_reply.started":"2021-05-22T22:49:48.096248Z","shell.execute_reply":"2021-05-22T22:49:54.978991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean lower\n\ndef clean_lower(df):\n    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: x.lower())\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-05-22T22:49:58.248993Z","iopub.execute_input":"2021-05-22T22:49:58.249676Z","iopub.status.idle":"2021-05-22T22:49:58.254442Z","shell.execute_reply.started":"2021-05-22T22:49:58.249638Z","shell.execute_reply":"2021-05-22T22:49:58.253284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean punctuation\n\npuncts = [\n    ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&',\n    '/', '[', ']', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£',\n    '·', '_', '{', '}', '©', '^', '®', '`', '→', '°', '€', '™', '›',\n    '♥', '←', '×', '§', '″', '′', 'Â', '█', 'à', '…', '“', '★', '”',\n    '–', '●', 'â', '►', '−', '¢', '¬', '░', '¶', '↑', '±',  '▾',\n    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '⊕', '▼',\n    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n    'è', '¸', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n    '¹', '≤', '‡', '₹', '´'\n]\n\ndef _clean_puncts(x, puncts):\n    x = str(x)\n    # added space around puncts after replace\n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, f' {punct} ')\n    return x\n\ndef clean_puncts(df, puncts):\n    df['question_text'] = df['question_text'].apply(lambda x: _clean_puncts(x, puncts))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-05-22T22:50:01.432965Z","iopub.execute_input":"2021-05-22T22:50:01.433361Z","iopub.status.idle":"2021-05-22T22:50:01.44679Z","shell.execute_reply.started":"2021-05-22T22:50:01.43333Z","shell.execute_reply":"2021-05-22T22:50:01.44498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean numbers\n\ndef _clean_numbers(x):\n if bool(re.search(r'\\d', x)):\n     x = re.sub('[0–9]{5,}', '#####', x)\n     x = re.sub('[0–9]{4}', '####', x)\n     x = re.sub('[0–9]{3}', '###', x)\n     x = re.sub('[0–9]{2}', '##', x)\n return x\n\ndef clean_numbers(df, puncts):\n    df['question_text'] = df['question_text'].apply(lambda x: _clean_numbers(x))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-05-22T22:50:07.512912Z","iopub.execute_input":"2021-05-22T22:50:07.513318Z","iopub.status.idle":"2021-05-22T22:50:07.520396Z","shell.execute_reply.started":"2021-05-22T22:50:07.513286Z","shell.execute_reply":"2021-05-22T22:50:07.519232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correcting mispelled words\n\nmispell_dict = {\n    \"colour\": \"color\", \n    \"centre\": \"center\", \n    \"favourite\": \"favorite\", \n    \"travelling\": \"traveling\", \n    \"counselling\": \"counseling\", \n    \"theatre\": \"theater\", \n    \"cancelled\": \"canceled\", \n    \"labour\": \"labor\", \n    \"organisation\": \"organization\", \n    \"wwii\": \"world war 2\", \n    \"citicise\": \"criticize\", \n    \"youtu \": \"youtube\", \n    \"Qoura\": \"Quora\", \n    \"sallary\": \"salary\", \n    \"Whta\": \"What\", \n    \"narcisist\": \"narcissist\", \n    \"howdo\": \"how do\", \n    \"whatare\": \"what are\", \n    \"howcan\": \"how can\", \n    \"howmuch\": \"how much\", \n    \"howmany\": \"how many\", \n    \"whydo\": \"why do\", \n    \"doI\": \"do I\", \n    \"theBest\": \"the best\", \n    \"howdoes\": \"how does\", \n    \"mastrubation\": \"masturbation\", \n    \"mastrubate\": \"masturbate\", \n    \"mastrubating\": \"masturbating\", \n    \"pennis\": \"penis\", \n    \"Etherium\": \"bitcoin\", \n    \"narcissit\": \"narcissist\", \n    \"bigdata\": \"big data\", \n    \"2k17\": \"2017\", \n    \"2k18\": \"2018\", \n    \"qouta\": \"quota\", \n    \"exboyfriend\": \"ex boyfriend\", \n    \"airhostess\": \"air hostess\", \n    \"whst\": \"what\", \n    \"watsapp\": \"whatsapp\", \n    \"demonitisation\": \"demonetization\", \n    \"demonitization\": \"demonetization\", \n    \"demonetisation\": \"demonetization\", \n    \"electroneum\": \"bitcoin\",\n    \"nanodegree\": \"degree\",\n    \"hotstar\": \"star\",\n    \"dream11\": \"dream\",\n    \"ftre\": \"fire\",\n    \"tensorflow\": \"framework\",\n    \"unocoin\": \"bitcoin\",\n    \"lnmiit\": \"limit\", \n    \"unacademy\": \"academy\",\n    \"altcoin\": \"bitcoin\",\n    \"altcoins\": \"bitcoin\", \n    \"litecoin\": \"bitcoin\",\n    \"coinbase\": \"bitcoin\",\n    \"cryptocurency\": \"cryptocurrency\",\n    \"simpliv\": \"simple\",\n    \"quoras\": \"quora\",\n    \"schizoids\": \"psychopath\",\n    \"remainers\": \"remainder\",\n    \"twinflame\": \"soulmate\",\n    \"quorans\": \"quora\",\n    \"brexit\": \"demonetized\",\n    \"iiest\": \"institute\",\n    \"dceu\": \"comics\",\n    \"pessat\": \"exam\", \n    \"uceed\": \"college\",\n    \"bhakts\": \"devotee\",\n    \"boruto\": \"anime\",\n    \"cryptocoin\": \"bitcoin\",\n    \"blockchains\": \"blockchain\",\n    \"fiancee\": \"fiance\",\n    \"redmi\": \"smartphone\",\n    \"oneplus\": \"smartphone\",\n    \"qoura\": \"quora\",\n    \"deepmind\": \"framework\",\n    \"ryzen\": \"cpu\",\n    \"whattsapp\": \"whatsapp\",\n    \"undertale\": \"adventure\",\n    \"zenfone\": \"smartphone\",\n    \"cryptocurencies\": \"cryptocurrencies\",\n    \"koinex\": \"bitcoin\",\n    \"zebpay\": \"bitcoin\",\n    \"binance\": \"bitcoin\",\n    \"whtsapp\": \"whatsapp\",\n    \"reactjs\": \"framework\",\n    \"bittrex\": \"bitcoin\",\n    \"bitconnect\": \"bitcoin\",\n    \"bitfinex\": \"bitcoin\",\n    \"yourquote\": \"your quote\",\n    \"whyis\": \"why is\",\n    \"jiophone\": \"smartphone\",\n    \"dogecoin\": \"bitcoin\",\n    \"onecoin\": \"bitcoin\", \n    \"poloniex\": \"bitcoin\",\n    \"7700k\": \"cpu\",\n    \"angular2\": \"framework\",\n    \"segwit2x\": \"bitcoin\",\n    \"hashflare\": \"bitcoin\", \n    \"940mx\": \"gpu\",\n    \"openai\": \"framework\",\n    \"hashflare\": \"bitcoin\",\n    \"1050ti\": \"gpu\",\n    \"nearbuy\": \"near buy\",\n    \"freebitco\": \"bitcoin\",\n    \"antminer\": \"bitcoin\",\n    \"filecoin\": \"bitcoin\", \n    \"whatapp\": \"whatsapp\",\n    \"empowr\": \"empower\",\n    \"1080ti\": \"gpu\",\n    \"crytocurrency\": \"cryptocurrency\",\n    \"8700k\": \"cpu\",\n    \"whatsaap\": \"whatsapp\",\n    \"g4560\": \"cpu\",\n    \"payymoney\": \"pay money\",\n    \"fuckboys\": \"fuck boys\",\n    \"intenship\": \"internship\",\n    \"zcash\": \"bitcoin\",\n    \"demonatisation\": \"demonetization\",\n    \"narcicist\": \"narcissist\",\n    \"mastuburation\": \"masturbation\",\n    \"trignometric\": \"trigonometric\",\n    \"cryptocurreny\": \"cryptocurrency\",\n    \"howdid\": \"how did\",\n    \"crytocurrencies\": \"cryptocurrencies\",\n    \"phycopath\": \"psychopath\",\n    \"bytecoin\": \"bitcoin\",\n    \"possesiveness\": \"possessiveness\",\n    \"scollege\": \"college\",\n    \"humanties\": \"humanities\",\n    \"altacoin\": \"bitcoin\",\n    \"demonitised\": \"demonetized\",\n    \"brasília\": \"brazilia\",\n    \"accolite\": \"accolyte\",\n    \"econimics\": \"economics\",\n    \"varrier\": \"warrier\",\n    \"quroa\": \"quora\",\n    \"statergy\": \"strategy\",\n    \"langague\": \"language\",\n    \"splatoon\": \"game\",\n    \"7600k\": \"cpu\",\n    \"gate2018\": \"gate 2018\",\n    \"in2018\": \"in 2018\",\n    \"narcassist\": \"narcissist\",\n    \"jiocoin\": \"bitcoin\",\n    \"hnlu\": \"hulu\",\n    \"7300hq\": \"cpu\",\n    \"weatern\": \"western\",\n    \"interledger\": \"blockchain\",\n    \"deplation\": \"deflation\", \n    \"cryptocurrencies\": \"cryptocurrency\", \n    \"bitcoin\": \"blockchain cryptocurrency\"\n}\n\ndef _correct_mispell(x, compiled_re, replace):\n    return compiled_re.sub(replace, x)\n\ndef correct_mispell(df, mispell_dict):\n    mispelled_word = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    def replace(match):\n        return mispell_dict[match.group(0)]\n    df['question_text'] = df['question_text'].apply(\n        lambda x: _correct_mispell(x, mispelled_word, replace)\n    )\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-05-22T22:50:10.479669Z","iopub.execute_input":"2021-05-22T22:50:10.480593Z","iopub.status.idle":"2021-05-22T22:50:10.503493Z","shell.execute_reply.started":"2021-05-22T22:50:10.48053Z","shell.execute_reply":"2021-05-22T22:50:10.502305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing Contraction\n\nabbreviations = {\n    \"ain't\": \"is not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'll\": \"he will\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"I'd\": \"I would\",\n    \"I'd've\": \"I would have\",\n    \"I'll\": \"I will\",\n    \"I'll've\": \"I will have\",\n    \"I'm\": \"I am\",\n    \"I've\": \"I have\",\n    \"i'd\": \"i would\",\n    \"i'd've\": \"i would have\",\n    \"i'll\": \"i will\",\n    \"i'll've\": \"i will have\",\n    \"i'm\": \"i am\",\n    \"i've\": \"i have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so as\",\n    \"this's\": \"this is\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"here's\": \"here is\",\n    \"they'd\": \"they would\",\n     \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n     \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\",\n    \"you'll've\": \"you will have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\",\n    \"who'd\": \"who would\",\n    \"who're\": \"who are\",\n    \"'re\": \" are\",\n    \"tryin'\": \"trying\",\n    \"doesn'\": \"does not\",\n    'howdo': 'how do',\n    'whatare': 'what are',\n    'howcan': 'how can',\n    'howmuch': 'how much',\n    'howmany': 'how many',\n    'whydo': 'why do',\n    'doI': 'do I',\n    'theBest': 'the best',\n    'howdoes': 'how does',\n}\n\ndef _clean_abreviation(x, compiled_re, replace):\n    return compiled_re.sub(replace, x)\n\ndef clean_abbreviation(df, abbreviations):\n    compiled_abbreviation = re.compile('(%s)' % '|'.join(abbreviations.keys()))\n    def replace(match):\n        return abbreviations[match.group(0)]\n    df['question_text'] = df[\"question_text\"].apply(\n        lambda x: _clean_abreviation(x, compiled_abbreviation, replace)\n    )\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-05-22T22:50:19.111683Z","iopub.execute_input":"2021-05-22T22:50:19.112269Z","iopub.status.idle":"2021-05-22T22:50:19.131474Z","shell.execute_reply.started":"2021-05-22T22:50:19.112234Z","shell.execute_reply":"2021-05-22T22:50:19.130469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove stopword\n\nimport nltk\nstopword_list = nltk.corpus.stopwords.words(\"english\")\n\ndef _remove_stopwords(text, is_lower_case=True):\n tokenizer = ToktokTokenizer()\n tokens = tokenizer.tokenize(text)\n tokens = [token.strip() for token in tokens]\n if is_lower_case:\n     filtered_tokens = [token for token in tokens if token not in stopword_list]\n else:\n     filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n filtered_text = \" \".join(filtered_tokens)\n return filtered_text\n\ndef remove_stopwords(df):\n    df['question_text'] = df['question_text'].apply(lambda x: _remove_stopwords(x))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-05-22T22:50:26.509389Z","iopub.execute_input":"2021-05-22T22:50:26.509952Z","iopub.status.idle":"2021-05-22T22:50:28.139582Z","shell.execute_reply.started":"2021-05-22T22:50:26.509915Z","shell.execute_reply":"2021-05-22T22:50:28.13878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean math\n\ndef _clean_math(x, compiled_re):\n    return compiled_re.sub(' <math> ', x)\n\ndef clean_math(df):\n    math_puncts = 'θπα÷⁴≠β²¾∫≥⇒¬∠＝∑Φ√½¼'\n    math_puncts_long = [r'\\\\frac', r'\\[math\\]', r'\\[/math\\]', r'\\\\lim']\n    compiled_math = re.compile('(%s)' % '|'.join(math_puncts))\n    compiled_math_long = re.compile('(%s)' % '|'.join(math_puncts_long))\n    df['question_text'] = df['question_text'].apply(lambda x: _clean_math(x, compiled_math_long))\n    df['question_text'] = df['question_text'].apply(lambda x: _clean_math(x, compiled_math))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-05-22T22:50:32.049159Z","iopub.execute_input":"2021-05-22T22:50:32.04995Z","iopub.status.idle":"2021-05-22T22:50:32.057059Z","shell.execute_reply.started":"2021-05-22T22:50:32.049896Z","shell.execute_reply":"2021-05-22T22:50:32.055867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stemming\n\nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize.toktok import ToktokTokenizer\n\ndef _stem_text(text):\n tokenizer = ToktokTokenizer()\n stemmer = SnowballStemmer(\"english\")\n tokens = tokenizer.tokenize(text)\n tokens = [token.strip() for token in tokens]\n tokens = [stemmer.stem(token) for token in tokens]\n return \" \".join(tokens)\n\ndef stem_text(df):\n    df['question_text'] = df['question_text'].apply(lambda x: _stem_text(x))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-05-22T22:50:35.000889Z","iopub.execute_input":"2021-05-22T22:50:35.001409Z","iopub.status.idle":"2021-05-22T22:50:35.008387Z","shell.execute_reply.started":"2021-05-22T22:50:35.001375Z","shell.execute_reply":"2021-05-22T22:50:35.007542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lemmatization\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize.toktok import ToktokTokenizer\n\nwordnet_lemmatizer = WordNetLemmatizer()\ndef _lemma_text(text):\n tokenizer = ToktokTokenizer()\n tokens = tokenizer.tokenize(text)\n tokens = [token.strip() for token in tokens]\n tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n return \" \".join(tokens)\n\ndef lema_text(df):\n    df['question_text'] = df['question_text'].apply(lambda x: _lemma_text(x))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-05-22T22:50:38.209501Z","iopub.execute_input":"2021-05-22T22:50:38.210067Z","iopub.status.idle":"2021-05-22T22:50:38.217122Z","shell.execute_reply.started":"2021-05-22T22:50:38.210031Z","shell.execute_reply":"2021-05-22T22:50:38.216142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to clean dataset\n\ndef clean(df):\n    df = clean_lower(df)\n    df = clean_puncts(df, puncts)\n    df = clean_numbers(df, puncts)\n    df = correct_mispell(df, mispell_dict)\n    df = clean_abbreviation(df, abbreviations)\n    df = remove_stopwords(df)\n    df = clean_math(df)\n    df = stem_text(df)\n    df = lema_text(df)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-05-22T22:50:41.224894Z","iopub.execute_input":"2021-05-22T22:50:41.225587Z","iopub.status.idle":"2021-05-22T22:50:41.23275Z","shell.execute_reply.started":"2021-05-22T22:50:41.22554Z","shell.execute_reply":"2021-05-22T22:50:41.231441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to run cleaning process\n\nfrom multiprocessing import Pool\nimport re\n\nnum_cores = 2\ndef df_parallelize_run(df, func, num_cores=2):\n    df_split = np.array_split(df, num_cores)\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-05-22T22:50:46.380742Z","iopub.execute_input":"2021-05-22T22:50:46.381257Z","iopub.status.idle":"2021-05-22T22:50:46.387153Z","shell.execute_reply.started":"2021-05-22T22:50:46.381224Z","shell.execute_reply":"2021-05-22T22:50:46.386216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleaning dataset\n\ntrain_data = df_parallelize_run(train_data, clean)\ntest_data = df_parallelize_run(test_data, clean)\nprint(\"Train shape : \", train_data.shape)\nprint(\"Test shape : \", test_data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T22:50:49.06924Z","iopub.execute_input":"2021-05-22T22:50:49.069739Z","iopub.status.idle":"2021-05-22T22:58:05.877951Z","shell.execute_reply.started":"2021-05-22T22:50:49.069708Z","shell.execute_reply":"2021-05-22T22:58:05.876199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Divide dataset\n\nX = train_data[\"question_text\"].values\ny = train_data[\"target\"].values","metadata":{"execution":{"iopub.status.busy":"2021-05-22T22:59:04.441171Z","iopub.execute_input":"2021-05-22T22:59:04.441776Z","iopub.status.idle":"2021-05-22T22:59:04.446461Z","shell.execute_reply.started":"2021-05-22T22:59:04.441731Z","shell.execute_reply":"2021-05-22T22:59:04.444997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize dataset\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\n\ntrunc_type='post'\npadding_type='post'\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X)\n\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, dtype='int64', padding='post')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T22:59:06.089297Z","iopub.execute_input":"2021-05-22T22:59:06.089772Z","iopub.status.idle":"2021-05-22T22:59:59.15974Z","shell.execute_reply.started":"2021-05-22T22:59:06.089734Z","shell.execute_reply":"2021-05-22T22:59:59.158425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split dataset into training and testing samples\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=37)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:00:48.03781Z","iopub.execute_input":"2021-05-22T23:00:48.038203Z","iopub.status.idle":"2021-05-22T23:00:49.218687Z","shell.execute_reply.started":"2021-05-22T23:00:48.038169Z","shell.execute_reply":"2021-05-22T23:00:49.217006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get vocabulary size\n\nvocab_size = len(tokenizer.word_index) + 2","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:00:50.985828Z","iopub.execute_input":"2021-05-22T23:00:50.986586Z","iopub.status.idle":"2021-05-22T23:00:50.991205Z","shell.execute_reply.started":"2021-05-22T23:00:50.986549Z","shell.execute_reply":"2021-05-22T23:00:50.989794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build up model\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Embedding(vocab_size, 64))\nmodel.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,)))\n\n # One or more dense layers.\n# Edit the list in the `for` line to experiment with layer sizes.\nfor units in [64, 64]:\n  model.add(tf.keras.layers.Dense(units, activation='relu'))\n\n# Output layer. The first argument is the number of labels.\nmodel.add(tf.keras.layers.Dense(2, activation='softmax'))\nprint(model.summary()) ","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:00:53.80915Z","iopub.execute_input":"2021-05-22T23:00:53.809503Z","iopub.status.idle":"2021-05-22T23:00:54.639811Z","shell.execute_reply.started":"2021-05-22T23:00:53.809474Z","shell.execute_reply":"2021-05-22T23:00:54.638281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add optimizer\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:00:59.808823Z","iopub.execute_input":"2021-05-22T23:00:59.809212Z","iopub.status.idle":"2021-05-22T23:00:59.827942Z","shell.execute_reply.started":"2021-05-22T23:00:59.809181Z","shell.execute_reply":"2021-05-22T23:00:59.826806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training model\n\nhistory= model.fit(X_train, y_train,validation_split=0.1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:01:03.041255Z","iopub.execute_input":"2021-05-22T23:01:03.041677Z","iopub.status.idle":"2021-05-23T01:27:35.634977Z","shell.execute_reply.started":"2021-05-22T23:01:03.041644Z","shell.execute_reply":"2021-05-23T01:27:35.633974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save model\n\nmodel.save('../output/kaggle/working/quora_prediction.h5')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:52:25.999286Z","iopub.execute_input":"2021-05-22T01:52:25.999717Z","iopub.status.idle":"2021-05-22T01:52:26.399683Z","shell.execute_reply.started":"2021-05-22T01:52:25.999657Z","shell.execute_reply":"2021-05-22T01:52:26.398773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reconstructed_model = tf.keras.models.load_model('../output/kaggle/working/quora_prediction.h5')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T13:43:49.242809Z","iopub.execute_input":"2021-05-22T13:43:49.243304Z","iopub.status.idle":"2021-05-22T13:43:52.484792Z","shell.execute_reply.started":"2021-05-22T13:43:49.243275Z","shell.execute_reply":"2021-05-22T13:43:52.483843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction on testing sample\n\nprediction = reconstructed_model.predict(X_test)\nprint(prediction)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T13:45:11.544672Z","iopub.execute_input":"2021-05-22T13:45:11.545091Z","iopub.status.idle":"2021-05-22T13:47:04.828152Z","shell.execute_reply.started":"2021-05-22T13:45:11.54506Z","shell.execute_reply":"2021-05-22T13:47:04.827181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get label\n\ndef get_label(word_index):\n  if prediction[word_index][0] > prediction[word_index][1]:\n    return 0\n  else:\n    return 1 \n\ny_pred = [get_label(i) for i in range(len(X_test))]","metadata":{"execution":{"iopub.status.busy":"2021-05-22T13:47:35.89544Z","iopub.execute_input":"2021-05-22T13:47:35.89602Z","iopub.status.idle":"2021-05-22T13:47:35.900614Z","shell.execute_reply.started":"2021-05-22T13:47:35.895988Z","shell.execute_reply":"2021-05-22T13:47:35.89968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\ncnf_matrix = confusion_matrix(y_pred, y_test)\n\nimport matplotlib.pyplot as plt\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1, keepdims = True)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Plot non-normalized confusion matrix\nclass_names = [0, 1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names,\n                      title='Confusion matrix, without normalization')\nplt.show()\n\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:07:00.847393Z","iopub.execute_input":"2021-05-22T14:07:00.847739Z","iopub.status.idle":"2021-05-22T14:07:01.53873Z","shell.execute_reply.started":"2021-05-22T14:07:00.847709Z","shell.execute_reply":"2021-05-22T14:07:01.537849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Caculate critical scores\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\n# Caculate accuracy score \naccuracy = accuracy_score(y_pred, y_test)\nprint('Accuracy: %f' % accuracy)\n\n# Caculate precision score\nprecision = precision_score(y_pred, y_test)\nprint('Precision: %f' % precision)\n\n# Caculate recall score\nrecall = recall_score(y_pred, y_test)\nprint('Recall: %f' % recall)\n\n# Caculate F1 score\nf1 = f1_score(y_pred, y_test)\nprint('F1 score: %f' % f1)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T13:47:41.546859Z","iopub.execute_input":"2021-05-22T13:47:41.547184Z","iopub.status.idle":"2021-05-22T13:47:42.138765Z","shell.execute_reply.started":"2021-05-22T13:47:41.547159Z","shell.execute_reply":"2021-05-22T13:47:42.1378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}